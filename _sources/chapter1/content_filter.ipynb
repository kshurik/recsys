{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61df345",
   "metadata": {},
   "source": [
    "(chapter1_part3)=\n",
    "\n",
    "# Content-based Filtering in a Nutshell\n",
    "In this section, we will go through a straightforward way to generate candidates for recommendations.\n",
    "As we mentioned before, one of the methods is *content-based* filtering. We will go through an explanation of\n",
    "this method with an example and finally discuss a particular library to implement it.\n",
    "Before that, we have to define and understand embeddings. As you might have noticed, we mentioned a lot\n",
    "\"similar items\", \"similar users\" etc and the question arises -- how do we define that similarity?\n",
    "Speaking of the calculation of similarity it is pretty straightforward -- we calculate cosine between two arrays.\n",
    "The intriguing part is how we get these arrays from our data.\n",
    "\n",
    "## Embeddings Explained\n",
    "The evolution of text processing started from one-hot encoding. When there was text data, Data Scientists\n",
    "would preprocess them (lower case, remove symbols, etc.) and then create one-hot representations of words or\n",
    "n-grams (when we split words/text into 2-3-...-n parts by characters). Finally, use some ML model on top of it.\n",
    "Notwithstanding the fact of easiness and interpretability of this approach, human language is sophisticated\n",
    "and various words can mean different meanings depending on the context and such a technique fails in most cases.\n",
    "\n",
    "Therefore, embeddings have become the next stage in the text processing pipeline. It is the type of word representation\n",
    "that allows words with similar meanings to have a similar representation. Unlike methods such as one-hot encoding,\n",
    "word embeddings provide a way to represent words in a more meaningful way, by mapping them to a vector of real\n",
    "numbers in a continuous vector space. The idea behind word embedding is to use a neural network to learn\n",
    "relationships between words in a dataset. The neural network is trained to assign a numeric vector to each word\n",
    "in the dataset. Typically, the vector is of fixed length and the goal is to find a vector that accurately\n",
    "represents the meaning of the word, in the context of the dataset. This allows for words in similar contexts\n",
    "to have similar vector representations. \n",
    "\n",
    "For example, imagine a dataset of movie reviews. Let’s say that the neural network has been trained to assign\n",
    "a vector to each word in the dataset. If the word “amazing” is used in a movie review, then the vector assigned\n",
    "to “amazing” will be similar to the vector assigned to “incredible”. This is because the meanings of these two\n",
    "words are similar and they are often used in similar contexts. Word embeddings can also be used to identify\n",
    "relationships between words. For example, consider the words “man” and “woman”. If the neural network assigned\n",
    "similar vectors to these two words, this would indicate that the two words are related.  In addition to\n",
    "identifying relationships between words, word embeddings can also be used to classify documents. For example,\n",
    "if a document contains the words “amazing” and “incredible”, then the neural network can assign an appropriate\n",
    "vector to each of these words. If a second document contains similar words, then the neural network can assign\n",
    "similar vectors to these words. This allows the neural network to accurately classify the documents as being similar. \n",
    "\n",
    "Finally, word embeddings can be used for data visualization. By plotting the vectors assigned to words in\n",
    "a two-dimensional space, it is possible to see how words are related. This can be a useful tool for understanding\n",
    "the relationships between words in a given dataset. In summary, word embeddings are a powerful tool\n",
    "for representing words in a meaningful way. They can be used to identify relationships between words,\n",
    "classify documents, and visualize data. \n",
    "\n",
    "Now, let's consider *content-based filtering* and use simple Word2Vec/Doc2Vec\n",
    "model to get such recommendations.\n",
    "\n",
    "## Content-based Filtering\n",
    "Content-based filtering can be used in a variety of applications, from recommending films and music to suggesting\n",
    "restaurants and travel destinations. In this part, we'll discuss how content-based filtering works and provide\n",
    "some examples.\n",
    "\n",
    "Content-based filtering is a type of recommender system that recommends items to users based on their past\n",
    "preferences and behaviors. It works by analyzing a user's preferences, in terms of attributes such as genre,\n",
    "director, actor, or even a combination of these, and then recommending other items that have similar attributes.\n",
    "For example, if a user has previously watched romantic comedies with Julia Roberts, content-based filtering\n",
    "would recommend other romantic comedies with Julia Roberts, or other films featuring similar actors or directors.\n",
    "\n",
    "Content-based filtering is based on the assumption that users who liked one item will likely like similar items.\n",
    "To generate recommendations, the system first identifies the attributes of the items that the user has previously\n",
    "interacted with. It then identifies other items that have similar attributes and recommends them to the user.\n",
    "For example, if a user has previously listened to Taylor Swift songs, the system will identify other Taylor Swift\n",
    "songs as well as songs with similar attributes, such as a similar genre or artist. In industry, this type of\n",
    "recommendation is shown with \"Similar to ...\". It is an additional nudge to increase the interest of a user\n",
    "as recommendations with explanations seem to be personalized from the user's point of view.\n",
    "\n",
    "In conclusion, content-based filtering is a type of recommender system that recommends items to users based on their\n",
    "past preferences and behaviors. Next, we jump to the coding part and create a simple Word2Vec model via [`gensim`](https://pypi.org/project/gensim/) library.\n",
    "Well-explained the logic of the Word2Vec model you can find [here](https://jalammar.github.io/illustrated-word2vec/).\n",
    "Here, we will not discuss the details of implementation.\n",
    "\n",
    "### gensim: example of content-based recommendations based on Doc2Vec approach\n",
    "Now, we move on to the implementation of a content-based recommender using `gensim` library and Doc2Vec. It is almost\n",
    "the same as Word2Vec with slight modifications, but the idea remains the same.\n",
    "\n",
    "#### 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b354c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links to shared data MovieLens\n",
    "# source on kaggle: https://www.kaggle.com/code/quangnhatbui/movie-recommender/data\n",
    "MOVIES_METADATA_URL = 'https://drive.google.com/file/d/19g6-apYbZb5D-wRj4L7aYKhxS-fDM4Fb/view?usp=share_link'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c3706",
   "metadata": {},
   "source": [
    "#### 1. Modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc243d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/runner/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to make it available to download w/o SSL verification\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from ast import literal_eval\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# download stop words beforehand\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bff351",
   "metadata": {},
   "source": [
    "##### 1.1. Helper functions to avoid copypaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c370f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_gdrive(url):\n",
    "    \"\"\"\n",
    "    gets csv data from a given url (taken from file -> share -> copy link)\n",
    "    :url: example https://drive.google.com/file/d/1BlZfCLLs5A13tbNSJZ1GPkHLWQOnPlE4/view?usp=share_link\n",
    "    \"\"\"\n",
    "    file_id = url.split('/')[-2]\n",
    "    file_path = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1513ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to /home/runner/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# init lemmatizer to avoid slow performance\n",
    "mystem = Mystem() \n",
    "\n",
    "def word_tokenize_clean(doc: str, stop_words: list):\n",
    "    '''\n",
    "    tokenize from string to list of words\n",
    "    '''\n",
    "\n",
    "    # split into lower case word tokens \\w lemmatization\n",
    "    tokens = list(set(mystem.lemmatize(doc.lower())))\n",
    "  \n",
    "    # remove tokens that are not alphabetic (including punctuation) and not a stop word\n",
    "    tokens = [word for word in tokens if word.isalpha() and not word in stop_words \\\n",
    "              not in list(punctuation)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5fbf5",
   "metadata": {},
   "source": [
    "#### 2. Main\n",
    "##### 2.1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607a0e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adult                     object\n",
       "belongs_to_collection     object\n",
       "budget                    object\n",
       "genres                    object\n",
       "homepage                  object\n",
       "id                        object\n",
       "imdb_id                   object\n",
       "original_language         object\n",
       "original_title            object\n",
       "overview                  object\n",
       "popularity                object\n",
       "poster_path               object\n",
       "production_companies      object\n",
       "production_countries      object\n",
       "release_date              object\n",
       "revenue                  float64\n",
       "runtime                  float64\n",
       "spoken_languages          object\n",
       "status                    object\n",
       "tagline                   object\n",
       "title                     object\n",
       "video                     object\n",
       "vote_average             float64\n",
       "vote_count               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv information about films etc\n",
    "movies_metadata = read_csv_from_gdrive(MOVIES_METADATA_URL)\n",
    "movies_metadata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ed431",
   "metadata": {},
   "source": [
    "To get accurate results we need to preprocess the text a bit. The pipeline will be as follows:\n",
    "- Filter only necessary columns from movies_metadada : id, original_title, overview;\n",
    "- Define `model_index` for the model to match back with `id` column;\n",
    "- Text cleaning: removing stopwords & punctuation, lemmatization for further tokenization, and tagged document creation required for gensim.Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd119e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45466 entries, 0 to 45465\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              45466 non-null  object\n",
      " 1   original_title  45466 non-null  object\n",
      " 2   overview        44512 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# filter cols\n",
    "sample = movies_metadata[['id', 'original_title', 'overview']].copy()\n",
    "sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e196af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "original_title    0\n",
       "overview          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you see from above, we have missing overview in some cases -- let's fill it with the original title\n",
    "sample.loc[sample['overview'].isnull(), 'overview'] = sample.loc[sample['overview'].isnull(), 'original_title']\n",
    "sample.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf1a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model_index and make it as string\n",
    "sample = sample.reset_index().rename(columns = {'index': 'model_index'})\n",
    "sample['model_index'] = sample['model_index'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231173ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapper with title and model_idnex to use it further in evaluation\n",
    "movies_inv_mapper = dict(zip(sample['original_title'].str.lower(), sample['model_index'].astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63b37e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess by removing non-character data, stopwords\n",
    "tags_corpus = sample['overview'].values\n",
    "tags_corpus = [re.sub('-[!/()0-9]', '', x) for x in tags_corpus]\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "tags_doc = [word_tokenize_clean(description, stop_words) for description in tags_corpus]\n",
    "tags_corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de831adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data as model input for Word2Vec\n",
    "## it takes some time to execute\n",
    "tags_doc = [TaggedDocument(words = word_tokenize_clean(D, stop_words), tags = [str(i)]) for i, D in enumerate(tags_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b4ff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['siblings', 'enchanted', 'unwittingly', 'invite', 'game', 'years', 'find', 'running', 'trapped', 'discover', 'judy', 'inside', 'room', 'board', 'risky', 'monkeys', 'magical', 'finish', 'alan', 'evil', 'rhinoceroses', 'proves', 'terrifying', 'living', 'door', 'three', 'world', 'creatures', 'hope', 'opens', 'giant', 'freedom', 'peter', 'adult'], tags=['1'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check what do we have\n",
    "## tag = movie index\n",
    "tags_doc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d132e6",
   "metadata": {},
   "source": [
    "#### 2.2. Model Training and Evaluation\n",
    "\n",
    "First, let's define some paramters for Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79104dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC_SIZE = 50 # length of the vector for each movie\n",
    "ALPHA = .02 # model learning param\n",
    "MIN_ALPHA = .00025 # model learning param\n",
    "MIN_COUNT = 5 # min occurrence of a word in dictionary\n",
    "EPOCHS = 20 # number of trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d06bf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = Doc2Vec(vector_size = VEC_SIZE,\n",
    "                alpha = ALPHA, \n",
    "                min_alpha = MIN_ALPHA,\n",
    "                min_count = MIN_COUNT,\n",
    "                dm = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d937d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vocab from all tag docs\n",
    "model.build_vocab(tags_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f931ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model.train(tags_doc,\n",
    "            total_examples = model.corpus_count,\n",
    "            epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559d0d5",
   "metadata": {},
   "source": [
    "Now, let's make some checks by defining parameters for the model ourselves.\n",
    "Assume that we watched the movie `batman` and based on that generate recommendations similar to its description.\n",
    "To do that we need:\n",
    "- To extract movie id from `movies_inv_mapper` we created to map back titles from the model output\n",
    "- Load embeddings from the trained model\n",
    "- Use the built-in most_similar() method to get the most relevant recommendations based on film embedding\n",
    "- Finally, map title names for sense-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2457e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get id\n",
    "movie_id = movies_inv_mapper['batman']\n",
    "movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3207be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained embeddings \n",
    "movies_vectors = model.dv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97414b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_embeddings = movies_vectors[movie_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b603d78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>model_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8603</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13835</td>\n",
       "      <td>0.955373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5713</td>\n",
       "      <td>0.955007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7772</td>\n",
       "      <td>0.952817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43461</td>\n",
       "      <td>0.947201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_index  model_score\n",
       "0        8603     1.000000\n",
       "1       13835     0.955373\n",
       "2        5713     0.955007\n",
       "3        7772     0.952817\n",
       "4       43461     0.947201"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get recommendations\n",
    "similars = model.docvecs.most_similar(positive = [movie_embeddings], topn = 20)\n",
    "output = pd.DataFrame(similars, columns = ['model_index', 'model_score'])\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b37f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse values and indices to map names in dataframe\n",
    "name_mapper = {v: k for k, v in movies_inv_mapper.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76567e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>model_score</th>\n",
       "      <th>title_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>batman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13835</td>\n",
       "      <td>0.955373</td>\n",
       "      <td>k2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5713</td>\n",
       "      <td>0.955007</td>\n",
       "      <td>rollover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7772</td>\n",
       "      <td>0.952817</td>\n",
       "      <td>this island earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43461</td>\n",
       "      <td>0.947201</td>\n",
       "      <td>megafault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1045</td>\n",
       "      <td>0.946845</td>\n",
       "      <td>sleeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44053</td>\n",
       "      <td>0.946433</td>\n",
       "      <td>arès</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33298</td>\n",
       "      <td>0.946160</td>\n",
       "      <td>necessary evil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12330</td>\n",
       "      <td>0.944601</td>\n",
       "      <td>the detonator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2551</td>\n",
       "      <td>0.944253</td>\n",
       "      <td>it conquered the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>43165</td>\n",
       "      <td>0.944148</td>\n",
       "      <td>the zookeeper's wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44339</td>\n",
       "      <td>0.942780</td>\n",
       "      <td>the underground world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29872</td>\n",
       "      <td>0.942076</td>\n",
       "      <td>angels die hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20807</td>\n",
       "      <td>0.942075</td>\n",
       "      <td>planet hulk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35181</td>\n",
       "      <td>0.941785</td>\n",
       "      <td>конек-горбунок</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>42040</td>\n",
       "      <td>0.939629</td>\n",
       "      <td>equalizer 2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10227</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>stealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22017</td>\n",
       "      <td>0.939572</td>\n",
       "      <td>outside the law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38015</td>\n",
       "      <td>0.939472</td>\n",
       "      <td>trailin' west</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44262</td>\n",
       "      <td>0.938266</td>\n",
       "      <td>quest of the delta knights</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_index  model_score                  title_name\n",
       "0         8603     1.000000                      batman\n",
       "1        13835     0.955373                          k2\n",
       "2         5713     0.955007                    rollover\n",
       "3         7772     0.952817           this island earth\n",
       "4        43461     0.947201                   megafault\n",
       "5         1045     0.946845                     sleeper\n",
       "6        44053     0.946433                        arès\n",
       "7        33298     0.946160              necessary evil\n",
       "8        12330     0.944601               the detonator\n",
       "9         2551     0.944253      it conquered the world\n",
       "10       43165     0.944148        the zookeeper's wife\n",
       "11       44339     0.942780       the underground world\n",
       "12       29872     0.942076             angels die hard\n",
       "13       20807     0.942075                 planet hulk\n",
       "14       35181     0.941785              конек-горбунок\n",
       "15       42040     0.939629              equalizer 2000\n",
       "16       10227     0.939624                     stealth\n",
       "17       22017     0.939572             outside the law\n",
       "18       38015     0.939472               trailin' west\n",
       "19       44262     0.938266  quest of the delta knights"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['title_name'] = output['model_index'].astype(int).map(name_mapper)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   11,
   89,
   93,
   96,
   118,
   120,
   133,
   149,
   154,
   158,
   165,
   170,
   176,
   182,
   187,
   197,
   203,
   207,
   212,
   220,
   229,
   234,
   239,
   249,
   255,
   260,
   264,
   271,
   276
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}