{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f36ec5",
   "metadata": {},
   "source": [
    "(chapter1_part4)=\n",
    "\n",
    "# Validation and Metrics\n",
    "In this chapter, we will go through the validation and evaluation of ML models in general and move\n",
    "on to specific metrics related to recommendation systems. First, we will define validation,\n",
    "why we need it, and what types are frequently used. Then, we will define some metrics with examples\n",
    "of calculation so it is convenient for everyone.\n",
    "\n",
    "## Validation Methods\n",
    "Training a recommendation system is a complex process, requiring careful consideration\n",
    "of several different aspects. One of the key steps in the recommendation system\n",
    "training pipeline is the validation process too. This process is essential for ensuring\n",
    "that the data used to train the model is accurate and that the model is performing as expected.\n",
    "\n",
    "Validation is a process that assesses the performance of the model and can help\n",
    "to detect any issues or biases in the data that could impact the performance of the model in\n",
    "production. Without validation, it is impossible to know whether the model is correctly\n",
    "capturing user preferences and providing accurate recommendations. The validation process\n",
    "also helps to detect any potential problems that could arise during the training process\n",
    "such as overfitting or under-fitting of the data, some errors in data collection, etc.\n",
    "\n",
    "In general, the validation process typically involves splitting the data into train and\n",
    "test sets. The train set is used to build and train the model, while the test set is\n",
    "used to measure the performance of the model and detect any issues in the data.\n",
    "The results of the validation process indicate how well the model\n",
    "can capture general patterns and provide accurate predictions.\n",
    "\n",
    "Validation can be carried out using many different methods such as the `holdout method`,\n",
    "`k-fold`, `stratified k-fold`, `leave-p-out`, and `time-based`. These methods are popular methods of\n",
    "validation in which the data is divided into multiple subsets which are then used in multiple\n",
    "rounds of training and testing.\n",
    "\n",
    "- `holdout method` - we just divide our data into three parts: train, test, and validation. Train will\n",
    "be used for model training, test for performance estimation, and validation as a final check on unseen data;\n",
    "\n",
    "- `k-fold` - we divide data into two sets: train and test. Then, we train the model using the only train. K-fold\n",
    "allows us to divide the train set into *k* subsets. Then we iterate over each subset and leave it as hold out set\n",
    "to estimate model performance and *k-1* is used for training;\n",
    "\n",
    "- `stratified k-fold` - it is similar to the classic *k-fold* with a modification that overcomes imbalanced targets.\n",
    "It samples data such that each fold have approximately the same number of distinct target values;\n",
    "\n",
    "- `leave-p-out` - we use *p* observations for test and *(n - p)* as train set. Once the training is done on\n",
    "*(n - p)*, *p* data points are used for validation.  All possible combinations of *p* are tested on the\n",
    "model to get the highest model performance\n",
    "\n",
    "All aforementioned approaches can be found in [scikit-learn library](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "Moreover, Machine Learning Simplified book provides a great overview of the methods [here](https://code.themlsbook.com/chapter5/validation_methods.html)\n",
    "\n",
    "- `time-based` - we need it more in time series problems. Intuitively, we define the training window and \n",
    "test window. Then, we go through our data with a sliding window - compute loss at each step - average in the end.\n",
    "Example of implementation is [here](https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8) and\n",
    "another implementation with cumulative approach in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)\n",
    "\n",
    "Finally, validation helps to ensure that the model is not biased in any way. This is especially\n",
    "important in the case of recommendation systems, as they are often used to recommend products and\n",
    "services to users. If the data used to train the model is biased, the modelâ€™s predictions may not\n",
    "reflect user preferences accurately, leading to an inaccurate and potentially unfair recommendation\n",
    "system. Validation helps to detect any potential bias in the data, which can then be addressed by\n",
    "adjusting the model parameters or using a different data set.\n",
    "So, what's the appropriate way for recommender systems? The answer is -- time-based split.\n",
    "We define a time interval for the test set and use all data up to the test set start date.\n",
    "However, it is for the first-level models. In practice, taking into account re-ranker,\n",
    "We have a more complicated split. Here are the steps:\n",
    "1. Create global train and test by time splitting;\n",
    "2. Use the global train and split again by time -- let's call it local_train and local_test;\n",
    "3. Use local_train to train the first-level model to generate candidates and predict on local_test;\n",
    "4. Use local_test and `split by users` into ranker_train, ranker_test to train ranker on ranker_train\n",
    "and validate on ranker_test;\n",
    "5. Finally, make predictions for the global test using the first-level from step 3 and reranker from step 4\n",
    "The scheme is defined below\n",
    "\n",
    "![](img/validation_scheme.png)\n",
    "\n",
    "The code below imitates splitting into global train and test by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f5bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "TRAIN_MAX_DATE = dt.datetime(2023, 2, 14) # define last date to include in train set\n",
    "TEST_INTERVAL_DAYS = 14 # define number of days to use for test\n",
    "TEST_MAX_DATE = TRAIN_MAX_DATE + dt.timedelta(days = TEST_INTERVAL_DAYS)\n",
    "\n",
    "# create artificial df\n",
    "df = pd.DataFrame({'date_time': [], 'values': []})\n",
    "\n",
    "global_train = df.loc[df['date_time'] <= TRAIN_MAX_DATE].reset_index(drop = True)\n",
    "global_test = df.loc[(df['date_time'] > TRAIN_MAX_DATE) \\\n",
    "                  & (df['date_time'] <= (TEST_MAX_DATE))].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8f646",
   "metadata": {},
   "source": [
    "Also, we should consider `cold \\ warm start` problems:\n",
    "- Cold Start - we do not have any interactions in train and test sets;\n",
    "- Warm Start - we did not have anything in the train set, but interactions appear during the test set\n",
    "\n",
    "## Metrics\n",
    "When developing machine learning models, evaluation metrics are an essential part of the process.\n",
    "There are a variety of metrics that can be used, each with its own benefits and drawbacks, and\n",
    "understanding them is key to creating successful models.\n",
    "\n",
    "Firstly, evaluation metrics allow us to measure the performance of our models. Without these metrics,\n",
    "we would not be able to gauge the success of our models, as there would be no objective way to \n",
    "measure how well our models were performing. Evaluation metrics also provide an objective method of\n",
    "comparing different models so that we can select the best one for the task at hand.\n",
    "\n",
    "Secondly, evaluation metrics can be used to identify the strengths and weaknesses of our models.\n",
    "By using such metrics we can identify which areas of our model are performing well, as well as\n",
    "which areas need improvement. This information can then be used to refine our models and improve their performance.\n",
    "\n",
    "Finally, evaluation metrics can be used to assess the generalizability of our models. In other words,\n",
    "how well the models will perform in unseen data. This is important for any machine learning model,\n",
    "as the ultimate goal is to create models that can generalize well and successfully make predictions\n",
    "on data that has not been seen before.\n",
    "\n",
    "### Regression\n",
    "Mean Absolute Error (`MAE`) = $\\frac{1}{N} \\sum_{i=1}^{D}|x_i-y_i|$\n",
    "\n",
    "Mean Squared Error (`MSE`) = $\\frac{1}{N} \\sum_{i=1}^{D}(x_i-y_i)^2$\n",
    "\n",
    "What is their baseline by the way? :)\n",
    "\n",
    "### Classification (Confusion Matrix)\n",
    "From classification tasks, we can use standard metrics that are widely used. Below, there is\n",
    "a well-known confusion matrix. Based on that matrix we can calculate various metrics like Precision\n",
    "and Recall. Their formulae and definition we will discuss later, but for now, let's elaborate on\n",
    "what each of the events means in terms of recommendations.\n",
    "\n",
    "|  |  Positive | Negative |\n",
    "|---|---|---|\n",
    "Positive | True Positive `(TP)` | False Positive `(FP)` |\n",
    "Negative | False Negative `(FN)` | True Negative `(TN)` |\n",
    "\n",
    "- `TP` - we recommended an item and user interacted;\n",
    "- `FP` - we recommended an item and user did not interact;\n",
    "- `FN` - we did not recommend an item, but user interacted with it;\n",
    "- `TN` - we did not recommend an item and user did not interact\n",
    "\n",
    "Now, let's define the most popular metrics for recommendations based on classification metrics - Precision@K & Recall&K.\n",
    "First, you need to understand what `@K` stands for. In recommendations, we return some list of items in a given order.\n",
    "Thus, we want to know, how many interactions we got from that list and therefore some threshold must be set to cut the\n",
    "list length. For example, we recommended 100 movies, but usually, users do not scroll more than 20 of them and we want\n",
    "to estimate our metric only on a subset of recommendations - top-20 positions and that would be Precision@20 & Recall@20.\n",
    "\n",
    "- `Precision@K` - share of relevant items in a list. Formula is $\\frac{TP}{TP + FP}$.\n",
    "Also, $TP + FP$ is *K* - total number of items and the formula simplifies to $\\frac{TP}{K}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6125af8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precission_at_k(y_true: np.array, y_pred: np.array, k: int) -> float:\n",
    "    \"\"\"\n",
    "    y_true: true labels\n",
    "    y_pred: predicted lables\n",
    "    k: cutoff length\n",
    "    \"\"\"\n",
    "\n",
    "    if sum(y_true) == 0:\n",
    "        return -1\n",
    "\n",
    "    argsort = np.argsort(y_pred)\n",
    "    y_true_sorted = y_true[argsort]\n",
    "    true_positives = y_true_sorted[:k].sum()\n",
    "\n",
    "    return true_positives / k\n",
    "\n",
    "# example array\n",
    "y_true = np.array([1, 0, 0, 1, 0, 0])\n",
    "y_pred = np.array([0, 0, 1, 1, 0, 0])\n",
    "\n",
    "# check\n",
    "precission_at_k(y_true, y_pred, k = 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76604f",
   "metadata": {},
   "source": [
    "- `Recall@K` - share of relevant items in a list of recommendations. Formula is $\\frac{TP}{TP + FN}$,\n",
    "where $TP + FN$ is number of known interactions (relevant items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79e4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE DONE WHEN HOMEWORK FROM STUDENTS IS SUBMITTED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4896769",
   "metadata": {},
   "source": [
    "### Ranking\n",
    "Using regression or classification metrics we evaluate the predicted values of the model, but not real relevance.\n",
    "In recommendations, we need both positive interaction and relevant items to be as high as possible. This is not\n",
    "possible using those metrics. Thus, ranking metrics have been incorporated for such tasks. In general,\n",
    "they consider both positive interactions with higher weights for those items that are higher in order.\n",
    "The most popular ones are `Mean Reciprocal Rank`, `Mean Average Precision`, and `Normalized Discounted Cumulative Gain`.\n",
    "\n",
    "- Mean Reciprocal Rank (`MRR`) is an average inverse rank. Formula is $\\frac{1}{N} \\sum_{i=1}^{N}\\frac{1}{rank_i}$\n",
    "\n",
    "| user_id | rekkos_list | interaction | rank | reciprocal rank |\n",
    "|---|---|---|---|---|\n",
    "| 1 | [batman, haryy potter, ozark] | batman | 1 | 1/1 |\n",
    "| 2 | [ozark, thor, something] | something | 3 | 1/3 |\n",
    "| 3 | [something, harry potter, batman] | None | 0 | 0 |\n",
    "\n",
    "Then, according to our formula $MRR = (\\frac{1}{1} + \\frac{1}{3} + 0) / 3 = 0.44$. Keep in mind that only\n",
    "the rank of the first relevant answer is considered, possible further relevant answers are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b105bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(y_true: np.array, y_pred: np.array) -> float:\n",
    "    \n",
    "    argsort = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = y_true[argsort]\n",
    "    for i, val in enumerate(y_true_sorted, 1):     \n",
    "        if val == 1:\n",
    "            return 1 / i\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04146c16",
   "metadata": {},
   "source": [
    "- Mean Average Precision at K (`MAP@K`) - average precision by users. Formula is divided into two parts:\n",
    "- Average Precision at K by user (`AP@K`) = $\\frac{1}{r_user} \\sum_{i=1}^{K}Precision@i * rel_i$,\n",
    "where $K$ - number of recommendations,  $r_user$ - number of releveant items for a user\n",
    "- $MAP@K = \\frac{1}{N} \\sum_{i=1}^{N}AP@K(user_i)$\n",
    "\n",
    "| user_id | movie | interaction | Precision@K |\n",
    "|---|---|---|---|\n",
    "| 1 | ozark | 1 | 1/1 |\n",
    "| 1 | batman | 0 | 1/2 |\n",
    "| 1 | harry | 0 | 1/3 |\n",
    "| 1 | thor | 1 | 2/4 |\n",
    "| 1 | something | 0 | 2/5 |\n",
    "| 1 |  something2 | 0 | 2/6 |\n",
    "\n",
    "AP@6 = $\\frac{1}{2} * (\\frac{1}{1} * 1 + \\frac{1}{2} * 0 + \\frac{1}{3} * 0 + \\frac{2}{4} * 1 + \\frac{2}{5} * 0 + \\frac{2}{6} * 0)$ = 0.75\n",
    "The total number of relevant items for a user is 2, therefore we multiply by 1/2.\n",
    "Looking at the first rank, we see that the user interacted with our recommendation and\n",
    "according to our formula, we get 1/1 and multiply by relevance 1. Then, in the following\n",
    "one we do not have interaction, our Precision@2 is 1/2, and multiplying by relevance 0\n",
    "we get 0. Further, we do the same logic and come to the resulting 0.75 MAP@6 (because we have only 1 user in the example).\n",
    "The code relies on `Precision@K` from the above, but if you want -- you can add it yourself.\n",
    "\n",
    "Now, what is MAP@3? :)\n",
    "\n",
    "- `Normalized Discounted Cumulative Gain (NDCG)` - averaged accuracy by users where only rank is needed to \n",
    "determine the metric. The formula is complex and looks like $\\mathrm{nDCG@K} = \\frac{DCG_{k}}{IDCG_{k}}$.\n",
    "Now, let's dive into numerator and denominator to understand the logic\n",
    "- Cumulative Gain (`CG`) is the sum of the graded relevance values of all results in a ranked result list.\n",
    "The value computed with the `CG` function is unaffected by changes in the ordering of ranked results. Thus,\n",
    "moving relevant item higher than irrelevant item does not change `CG` value\n",
    "Formula is $\\mathrm{CG_{k}} = \\sum_{i=1}^{k} rel_{i}$;\n",
    "As you can see, if we have a binary relevance label then it is exactly equal to Precision.\n",
    "- Discounted Cumulative Gain (`DCG`) - the premise of `DCG` is those relevant items that appear lower in a recommendation\n",
    "list result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\n",
    "Formula is $\\mathrm{DCG_{k}} = \\sum_{i=1}^{k} \\frac{ 2^{rel_{i}} - 1 }{ \\log_{2}(i+1)}$. This already gives a lot of information,\n",
    "but how do we know which value is good? To achieve that, we need to calculate what would be the best value\n",
    "for a given set of data. For that, Ideal DCG has been introduced\n",
    "- Ideal DCG (`IDCG`) is calculated as follows $\\mathrm{IDCG@K} = \\sum_{i=1}^{|REL_k|} \\frac{ rel_{i} }{ \\log_{2}(i+1)}$\n",
    "\n",
    "Let's consider an example, where we recommended 6 movies with relevance scores defined as [3, 2, 3, 0, 1, 2]. In the\n",
    "table, the calculation using the above formula is shown to get CG@6, DCG@6, IDCG@6, and finally NDCG@6.\n",
    "| $i$ | movie | $rel_{i}$ | $\\log_{2}(i+1)$ | $\\frac{ 2^{rel_{i}} - 1 }{ \\log_{2}(i+1)}$ |\n",
    "|---|---|---|---|---|\n",
    "| 1 | ozark | 3 | 1 | 7 |\n",
    "| 2 | batman| 2 | 1.585 | 1.893 |\n",
    "| 3 | harry | 3 | 2 | 3.5 |\n",
    "| 4 | thor | 0 | 2.322 | 0 |\n",
    "| 5 | something | 1 | 2.585 | 0.387 | \n",
    "| 6 |  something2 | 2 | 2.807 | 1.069 |\n",
    "\n",
    "Thus, $\\mathrm{DCG@6} = 7 + 1.893 + 3.5 + 0 + 0.387 + 1.069 = 13.849$ and $\\mathrm{IDCG@6}$ would be calculated as monotonically\n",
    "decreasing order by relevance [3, 3, 2, 2, 1, 0] and $\\mathrm{IDCG@6} = 7 + 4.416 + 1.5 + 1.292 + 0.387 + 0 = 14.595$\n",
    "and $\\mathrm{nDCG@K} = \\frac{DCG_{k}}{IDCG_{k}} = \\frac{13.849}{14.595} = 0.95$\n",
    "\n",
    "The code ofr $\\mathrm{nDCG@K} is divided into 3 parts:\n",
    "1. compute_gain() - it relates to our numerator. In our example we used $2^{rel_{i} - 1}$, but in reality we can set it to constant;\n",
    "2. dcg() - computes Discounted Cumulative Gain;\n",
    "3. ndcg() - final function that computes $\\mathrm{nDCG@K}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b720704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9488107485678985\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "def compute_gain(y_value: float, gain_scheme: str) -> float:\n",
    "    \n",
    "    gain = {'exp2': 2 ** y_value - 1,\n",
    "            'const': y_value}\n",
    "\n",
    "    return float(gain[gain_scheme])\n",
    "\n",
    "def dcg(y_true: np.array, y_pred: np.array, gain_scheme: str) -> float:\n",
    "    \n",
    "    dcg = 0\n",
    "    argsort = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = y_true[argsort]\n",
    "\n",
    "    for idx, val in enumerate(y_true_sorted, 1):\n",
    "        gain = compute_gain(val, gain_scheme)\n",
    "        dcg += gain / log2(idx + 1)\n",
    "        \n",
    "    return dcg\n",
    "\n",
    "def ndcg(y_true: np.array, ys_pred: np.array, gain_scheme: str = 'const') -> float:\n",
    "    \n",
    "    # pred dcg then we calc the same to find max possible\n",
    "    preds_dcg = dcg(y_true, ys_pred, gain_scheme)\n",
    "    max_possible_dcg = dcg(y_true, y_true, gain_scheme)\n",
    "\n",
    "    return preds_dcg / max_possible_dcg\n",
    "\n",
    "y_pred = np.array([6, 5, 4, 3, 2, 1]) # some score to sort\n",
    "y_true = np.array([3, 2, 3, 0, 1, 2])\n",
    "\n",
    "print(ndcg(y_true, y_pred, 'exp2'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b63317",
   "metadata": {},
   "source": [
    "Your turn, get NDCG@3 and modify code such that `@K` is considered :)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   11,
   88,
   102,
   159,
   185,
   190,
   192,
   212,
   221,
   283,
   318
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}