{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18881c2e",
   "metadata": {},
   "source": [
    "(chapter2_part1)=\n",
    "\n",
    "# What comes after ML model development?\n",
    "This chapter describes what is production code and how to write it using complementary tools\n",
    "such as `poetry`, `Makefile` and code styling modules like `pylint`, `black`, `isort` and `flake8`.\n",
    "Finally, we will define the architecture of our production microservice of RecSys we developed in\n",
    "[Chapter 1](https://rekkobook.com/chapter1/full_pipeline.html)\n",
    "\n",
    "\n",
    "After developing a machine learning (ML) model, the next step is to write production code\n",
    "that enables the model to function in a real-world environment. Production code is typically\n",
    "written in a programming language such as Python and executed on a server or virtual machine\n",
    "that is accessible to end users. The goal of production code is to ensure that the ML model\n",
    "performs as expected in a production environment, including handling real-world data, scaling\n",
    "to meet demand, and providing accurate and timely predictions.\n",
    "\n",
    "\n",
    "To ensure that production code is robust, scalable, and reliable, different skills are required\n",
    "than those needed for research or experimentation. The production code pipeline involves several steps,\n",
    "including:\n",
    "- Replicating the development environment for full reproducibility of results;\n",
    "- Data preparation such as collecting necessary input and calculating runtime features;\n",
    "- Inference and postprocessing that includes making final predictions and any necessary\n",
    "postprocessing of predictions, such as scaling\n",
    "\n",
    "\n",
    "In addition, strong coding requirements must be met which can be tracked and fixed using:\n",
    "- Use a consistent coding style and naming convention\n",
    "- Write clear and concise comments\n",
    "- Include error handling and logging\n",
    "- Use version control to track changes and collaborate with others\n",
    "- Write unit tests to ensure that the code works as expected\n",
    "\n",
    "Therefore, various tools besides model and Python code are used. Here, let's make an overview of these tools\n",
    "that is used in development today.\n",
    "\n",
    "\n",
    "# Developement tools\n",
    "## Poetry\n",
    "The first tool to use when creating a production service or even any ML project to have full reproducibility\n",
    "of the results is library version control. Previously, `requirements.txt` was very popular among developers\n",
    "and Data Scientists. It is pretty straightforward in usage -- add the name and version of the library with a new line\n",
    "\n",
    "```\n",
    "pandas==1.1.5\n",
    "numpy==1.23.5\n",
    "etc.\n",
    "```\n",
    "However, more advanced config management has been discovered [`poetry`](https://python-poetry.org/) which allows\n",
    "to use it for dependencies management and configuration file for popular dev tools (we will discuss it later).\n",
    "The main features are:\n",
    "- Dependencies management - resolves all dependencies for each library in the correct order. When using requirements.txt,\n",
    "developers have to manually manage the installation and updating of dependencies. This can be time-consuming\n",
    "and error-prone, especially when dealing with complex projects that have multiple dependencies. In contrast,\n",
    "Poetry automates most of the dependency management process, making it easier to install, update, and remove dependencies.;\n",
    "- Isolated virtualenv - automatically creates a virtual environment for reproducibility with a given Python version\n",
    "and libraries (no need to execute by hand, just one line of command in CLI);\n",
    "- CLI interaction to manage configuration files - intuitive commands to add/remove/install dependencies;\n",
    "- BONUS: some parameters for the `pylint` styling guide, etc. can be set within the same config!\n",
    "\n",
    "\n",
    "Now, let's discuss details about how to use it. To initialize the config we need to run the installation\n",
    "```\n",
    "curl -sSL https://install.python-poetry.org | python3 -\n",
    "```\n",
    "\n",
    "Then, we can initialize the poetry config by executing (assuming we want to create dependency management for the pre-existing project)\n",
    "```\n",
    "poetry init\n",
    "```\n",
    "After we run, several optional questions will be asked to create the `pyproject.toml` file -- the main configuration file.\n",
    "You can either fill it (for instance set the Python version) or just press enter to use suggested default values.\n",
    "\n",
    "Next, we add package names as following\n",
    "```\n",
    "poetry add pandas\n",
    "```\n",
    "or specific version\n",
    "```\n",
    "poetry add pandas==1.5.1\n",
    "```\n",
    "Further, it creates a virtual environment with all packages and you can use it to run your scripts/notebooks\n",
    "or run by using bash commands\n",
    "```\n",
    "poetry run python inference.py\n",
    "```\n",
    "\n",
    "Also, you can set various parameters by utilizing the full power of TOML extensions. Below there is an example of \n",
    "a full poetry set-up generated for illustration purposes.\n",
    "\n",
    "- `[tool.poetry]` - meta-information about the project\n",
    "- `[tool.poetry.dependencies]` - main dependencies used in production\n",
    "- `[tool.poetry.group.dev.dependencies]` - development packages only needed for test and code style guides\n",
    "- `[build-system]` - poetry system parameters\n",
    "- `[tool.isort], [tool.black], [tool.pylint]` - parameters to set up custom code style and checks.\n",
    "We will discuss them in the next part.\n",
    "\n",
    "```\n",
    "[tool.poetry]\n",
    "name = \"rekko_handbook\"\n",
    "version = \"0.1.0\"\n",
    "description = \"RecSys Handbook\"\n",
    "authors = [\"khalilbekov92@gmail.com\"]\n",
    "readme = \"README.md\"\n",
    "packages = [{include = \"rekko_handbook\"}]\n",
    "\n",
    "[tool.poetry.dependencies]\n",
    "python = \">=3.9,<3.12\"\n",
    "loguru = \"0.6.0\"\n",
    "pandas = \"1.5.3\"\n",
    "presto-python-client = \"0.8.3\"\n",
    "clickhouse-driver = \"0.2.5\"\n",
    "dynaconf = \"^3.1.12\"\n",
    "torch = \"^2.0.0\"\n",
    "transformers = \"^4.27.3\"\n",
    "tqdm = \"^4.65.0\"\n",
    "scipy = \"^1.10.1\"\n",
    "scikit-learn = \"^1.2.2\"\n",
    "boto3 = \"^1.26.106\"\n",
    "python-dotenv = \"^1.0.0\"\n",
    "\n",
    "[tool.poetry.group.dev.dependencies]\n",
    "pytest = \"^7.2.2\"\n",
    "pylint = \"^2.17.1\"\n",
    "flake8 = \"^6.0.0\"\n",
    "black = \"^23.1.0\"\n",
    "isort = \"^5.12.0\"\n",
    "pytest-html = \"^3.2.0\"\n",
    "\n",
    "[tool.isort]\n",
    "line_length = 120\n",
    "multi_line_output = 3\n",
    "include_trailing_comma = true\n",
    "\n",
    "[tool.black]\n",
    "line-length = 120\n",
    "target-version = ['py39']\n",
    "skip-string-normalization = true\n",
    "\n",
    "[build-system]\n",
    "requires = [\"poetry-core>=1.4.1\"]\n",
    "build-backend = \"poetry.core.masonry.api\"\n",
    "\n",
    "[tool.pylint.'FORMAT']\n",
    "min-similarity-lines = 10\n",
    "fail-under = 9.7\n",
    "py-version = 3.9\n",
    "good-names=[\n",
    "    'bp',\n",
    "    'db',\n",
    "    'df'\n",
    "]\n",
    "max-line-length = 120\n",
    "disable = [\n",
    "    'locally-disabled', 'suppressed-message',\n",
    "    'missing-module-docstring', 'missing-class-docstring',\n",
    "    'missing-function-docstring', 'too-few-public-methods',\n",
    "    'wrong-import-position', 'import-outside-toplevel',\n",
    "    'fixme', 'too-many-locals', 'too-many-arguments',\n",
    "    'too-many-instance-attributes', 'c-extension-no-member'\n",
    "]\n",
    "```\n",
    "\n",
    "Overall, poetry provides an easy and efficient way to reproduce development environment,\n",
    "resolve dependencies and allows using customization in some dev tools.\n",
    "\n",
    "## Styling guide and code quality\n",
    "As we have mentioned earlier, production code must be robust, scalable & reliable. Tracking\n",
    "the quality of the code by reviews only is exhaustive and almost impossible to achieve -- we\n",
    "are prone to errors too. However, most common mistakes like typos and convenient bugs can\n",
    "be checked automatically and fixed. Here, we will describe the main Python tools to achieve that\n",
    "- `[pylint]`(https://pypi.org/project/pylint/) - code analyzer which does not run your code.\n",
    "It checks for errors, and the coding standard makes clear suggestions on how to improve it\n",
    "and even grade it on a scale of 0 to 10;\n",
    "\n",
    "![](/img/pylint.png)\n",
    "\n",
    "- `[black]`(https://pypi.org/project/black/) - in the Python community there is well-known\n",
    "PEP-8 standard to follow. This library is a code formatter which is PEP 8-compliant\n",
    "opinionated formatter. It formats all code in given directory/files in place fast and efficiently;\n",
    "\n",
    "Before formatting with black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f45265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def some_kaif_function(input_df: pd.DataFrame, param_1: int, param_2: str, path: str):\n",
    "    \"\"\"\n",
    "    some example function\n",
    "    \"\"\"\n",
    "    input_df['check_bool'] = input_df.loc[(input_df[param_2] >= param_1) & (input_df[param_2] < param_1 - 1)]\n",
    "\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46529b74",
   "metadata": {},
   "source": [
    "After formatting with black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258ff71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def some_kaif_function(input_df: pd.DataFrame, param_1: int, param_2: str, path: str):\n",
    "    \"\"\"\n",
    "    some example function\n",
    "    \"\"\"\n",
    "    input_df[\"check_bool\"] = input_df.loc[\n",
    "        (input_df[param_2] >= param_1) & (input_df[param_2] < param_1 - 1)\n",
    "    ]\n",
    "    \n",
    "    return input_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6370db",
   "metadata": {},
   "source": [
    "- `[isort]`(https://pycqa.github.io/isort/) - a library to make appropriate imports:\n",
    "alphabetical order and group by types to sections. Below is the example from the official homepage\n",
    "\n",
    "Before the isort\n",
    "```\n",
    "from my_lib import Object\n",
    "\n",
    "import os\n",
    "\n",
    "from my_lib import Object3\n",
    "\n",
    "from my_lib import Object2\n",
    "\n",
    "import sys\n",
    "\n",
    "from third_party import lib15, lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11, lib12, lib13, lib14\n",
    "\n",
    "import sys\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from third_party import lib3\n",
    "```\n",
    "\n",
    "After isort\n",
    "```\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from third_party import (lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8,\n",
    "                         lib9, lib10, lib11, lib12, lib13, lib14, lib15)\n",
    "\n",
    "from my_lib import Object, Object2, Object3\n",
    "\n",
    "```\n",
    "\n",
    "## Makefile\n",
    "Makefiles are important tools for managing and automating the building and\n",
    "deployment of software projects. In Python development, Makefiles are especially\n",
    "useful in managing the compilation, packaging, and testing of code. A Makefile\n",
    "is essentially a script that defines a set of rules and actions to be performed\n",
    "when certain conditions are met. These rules can be used to automate repetitive\n",
    "tasks, such as compiling code, running tests, and generating documentation.\n",
    "\n",
    "They are particularly useful for large projects with many files and dependencies,\n",
    "as they allow developers to quickly and easily build, test, and deploy their code\n",
    "without having to manually type in commands for each step.\n",
    "\n",
    "For example, let's say you have a Python project with multiple modules, each with\n",
    "its dependencies. You would need to install these dependencies and set up\n",
    "the environment to run the project. With a Makefile, you can automate this\n",
    "process by defining targets for each step of the build,\n",
    "such as \"install dependencies\", \"set up environment\", \"run tests\", etc. \n",
    "\n",
    "Here's an example Makefile for a Python project:\n",
    "\n",
    "```\n",
    "# install dependencies\n",
    "install:\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "# set up the environment\n",
    "setup:\n",
    "    virtualenv env\n",
    "    source env/bin/activate\n",
    "\n",
    "# run tests\n",
    "test:\n",
    "    python -m unittest discover -s tests\n",
    "\n",
    "# clean up the environment\n",
    "clean:\n",
    "    rm -rf env/\n",
    "    @find . | grep __pycache__ | xargs rm -rf\n",
    "\n",
    "# let's add formatting stuff here as well\n",
    "\n",
    "pylint:\n",
    "    pylint app config tests # folder names\n",
    "\n",
    "isort:\n",
    "    isort app config tests --jobs=0\n",
    "\n",
    "black:\n",
    "    black app config tests\n",
    "\n",
    "fmt: isort black\n",
    "```\n",
    "\n",
    "This Makefile defines four targets: `install`, `setup`, `test`, and `clean`.\n",
    "- `make install` will install all the dependencies listed in `requirements.txt`;\n",
    "- `make setup` will create a virtual environment and activate it;\n",
    "- `make test` will run all the tests in the `tests` directory;\n",
    "- `make clean` will remove the virtual environment and any cached Python files;\n",
    "- `make pylint` will run a code formatting check;\n",
    "- `make isort` will do a proper import of modules;\n",
    "- `make black` will run formatting in place;\n",
    "- `make fmt` will call `make isort` & `make black` for full formatting\n",
    "\n",
    "Overall, Makefiles are important for Python development because they automate\n",
    "the many routine processes, making it easier and faster for developers\n",
    "to build, test, and deploy their code.\n",
    "\n",
    "# Architecture for our RecSys Project\n",
    "Let's recall the two-level architecture of a recommender system consists of two main components:\n",
    "the candidate generator and the ranker. The candidate generator is responsible for \n",
    "selecting a set of items that are likely to be of interest to the user, and the ranker\n",
    "takes those items and ranks them in order of predicted relevance to the user. Keeping\n",
    "that in mind, the architecture for the light version will be as follows:\n",
    "\n",
    "![](/img/recsys_architecture.png)\n",
    "\n",
    "Here is a detailed description of each component:\n",
    "1. `Client-user interface` to interact with the product;\n",
    "\n",
    "2. `First-level model a.k.a candidate Generator`: The candidate generator is the first level in the\n",
    "recommender system architecture. Its role is to select a set of items that are likely to be of\n",
    "interest to the user. It is usually designed to be very fast and efficient, as it needs to process\n",
    "a large number of potential items in a short amount of time.\n",
    "In the architecture above service described, the candidate generator receives a `user_id` as an\n",
    "input and uses a first-level model to select a set of candidate items that are likely to be of\n",
    "interest to that user. The output of the candidate generator is a set of movie ids;\n",
    "\n",
    "3. `FeatureStore` - a feature store is a centralized repository of features used in machine learning models.\n",
    "It enables easy sharing and reuse of features across different models and teams. Instead of storing features in\n",
    "separate databases, a feature store allows for efficient storage and retrieval of feature for inference.\n",
    "In this project, we will be using parquet files stored in GDrive as an example, rather than traditional\n",
    "databases. This will allow for easy integration and understanding key points of the project\n",
    "instead of technical stuff with data storage;\n",
    "\n",
    "4. `ReRanker`: The ranker is the second level in the recommender system architecture. Its role\n",
    "is to take the set of candidate items generated by the candidate generator and rank them in order\n",
    "of predicted relevance to the user. The ranker is usually a more complex and computationally\n",
    "expensive model than the candidate generator, as it needs to process a smaller set of potential\n",
    "items but with more detailed information\n",
    "\n",
    "In our production architecture, the ranker takes the set of candidate items generated\n",
    "by the first-level model and uses a feature store to enrich the data with additional information\n",
    "about the items and the user. This additional information could include item features such as genre,\n",
    "release date, or popularity, as well as user features such as demographics, past purchases,\n",
    "or browsing history. The ranker then applies a more sophisticated machine learning model to predict\n",
    "the relevance of each item to the user and sorts the items in order of predicted relevance.\n",
    "The output of the ranker is a sorted list of items that are likely to be of interest to the user.\n",
    "\n",
    "# How to structure the project\n",
    "Structuring an ML service is essential for a well-organized, maintainable, and scalable project.\n",
    "A well-structured project is easy to understand and easy to modify when requirements change.\n",
    "\n",
    "There are four base modules for structuring an ML service:\n",
    "- configs;\n",
    "- data preparation;\n",
    "- models;\n",
    "- utils\n",
    "\n",
    "Configs hold the parameters and settings of the project, data preparation contains the code\n",
    "for data processing and feature engineering, models contain the code for training, evaluating,\n",
    "and deploying models, and utils have common functions and classes that are frequently used throughout the project.\n",
    "\n",
    "\n",
    "`Configs` are an essential part of any ML project as they hold the settings and parameters of\n",
    "the project. Dynaconf is a popular Python package that simplifies working with configurations.\n",
    "Dynaconf enables developers to store configuration parameters in a file or an environment\n",
    "variable and read them in their code. An example of working with Dynaconf you can find in our project [here](https://github.com/kshurik/rekkobook/tree/chapter2/api_example/supplements/recsys/configs). It is a set of TOML files with parameters that\n",
    "are combined together using the Dynaconf loaders\n",
    "\n",
    "\n",
    "The `data preparation` module is responsible for loading and processing the input data before\n",
    "feeding it into the ML models. This module may contain functions for tasks such as data cleaning,\n",
    "normalization, feature engineering, and splitting the data into training and validation sets.\n",
    "Depending on the nature of the data and the specific ML problem being tackled, this module can be\n",
    "quite complex and may require its own submodules. \n",
    "\n",
    "\n",
    "The `models` module contains the actual ML models inference used to make predictions. This module may include\n",
    "different models for different tasks, as well as the code necessary to train and evaluate these models.\n",
    "In addition, this module may also include functions for loading and saving pre-trained models,\n",
    "as well as for fine-tuning models with new data.\n",
    "\n",
    "\n",
    "Finally, the `utils` module contains commonly used functions or classes shared across\n",
    "different parts of the service. This module may include functions for logging, visualization,\n",
    "file I/O, or any other utility functions that are not specific to either the data preparation or modeling modules.\n",
    "\n",
    "\n",
    "Thus, by separating the code into these four modules, the ML service can be organized in a logical\n",
    "and modular way, making it easier to maintain and extend over time."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "source_map": [
   11,
   195,
   206,
   209,
   223
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}